{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval augmented generative question answering with Pinecone (Updated for newest openai version and pinecone version 2025-01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref: https://cookbook.openai.com/examples/vector_databases/pinecone/gen_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU openai pinecone-client datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# print(openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=openai.api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you only have pairs of related sentences, you can use a Siamese network architecture for training sentence transformers. Siamese networks are designed to learn similarity between pairs of inputs, making them well-suited for tasks like sentence similarity or paraphrase detection. By training a Siamese network on your pairs of related sentences, you can learn a representation that captures the semantic similarity between sentences. This representation can then be used for tasks like information retrieval, question answering, or text classification.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = (\n",
    "    \"Which training method should I use for sentence transformers when \" +\n",
    "    \"I only have pairs of related sentences?\"\n",
    ")\n",
    "\n",
    "get_completion(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = \"text-embedding-ada-002\"\n",
    "\n",
    "res = openai.embeddings.create(\n",
    "    input=[\n",
    "        \"Sample document text goes here\",\n",
    "        \"there will be several phrases in each batch\"\n",
    "    ], \n",
    "    model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536, 1536)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res.data[0].embedding), len(res.data[1].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from transformers) (0.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nanfangwuyu\\.conda\\envs\\rag\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nanfangwuyu\\.conda\\envs\\RAG\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'published', 'url', 'video_id', 'channel_id', 'id', 'text', 'start', 'end'],\n",
       "    num_rows: 208619\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset('jamescalam/youtube-transcriptions', split='train')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       " 'published': '2021-07-06 13:00:03 UTC',\n",
       " 'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       " 'video_id': '35Pdoyi6ZoQ',\n",
       " 'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
       " 'id': '35Pdoyi6ZoQ-t0.0',\n",
       " 'text': 'Hi, welcome to the video.',\n",
       " 'start': 0.0,\n",
       " 'end': 9.36}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52155/52155 [01:18<00:00, 661.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "new_data = []\n",
    "\n",
    "window = 20  # number of sentences to combine\n",
    "stride = 4  # number of sentences to 'stride' over, used to create overlap\n",
    "\n",
    "for i in tqdm(range(0, len(data), stride)):\n",
    "    i_end = min(len(data)-1, i+window)\n",
    "    if data[i]['title'] != data[i_end]['title']:\n",
    "        # in this case we skip this entry as we have start/end of two videos\n",
    "        continue\n",
    "    text = ' '.join(data[i:i_end]['text'])\n",
    "    # create the new merged dataset\n",
    "    new_data.append({\n",
    "        'start': data[i]['start'],\n",
    "        'end': data[i_end]['end'],\n",
    "        'title': data[i]['title'],\n",
    "        'text': text,\n",
    "        'id': data[i]['id'],\n",
    "        'url': data[i]['url'],\n",
    "        'published': data[i]['published'],\n",
    "        'channel_id': data[i]['channel_id']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'start': 0.0,\n",
       "  'end': 74.12,\n",
       "  'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       "  'text': \"Hi, welcome to the video. So this is the fourth video in a Transformers from Scratch mini series. So if you haven't been following along, we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data.\",\n",
       "  'id': '35Pdoyi6ZoQ-t0.0',\n",
       "  'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       "  'published': '2021-07-06 13:00:03 UTC',\n",
       "  'channel_id': 'UCv83tO5cePwHMt1952IVVHw'},\n",
       " {'start': 15.84,\n",
       "  'end': 88.75999999999999,\n",
       "  'title': 'Training and Testing an Italian BERT - Transformers From Scratch #4',\n",
       "  'text': \"we've essentially covered what you can see on the screen. So we got some data. We built a tokenizer with it. And then we've set up our input pipeline ready to begin actually training our model, which is what we're going to cover in this video. So let's move over to the code. And we see here that we have essentially everything we've done so far. So we've built our input data, our input pipeline. And we're now at a point where we have a data loader, PyTorch data loader, ready. And we can begin training a model with it. So there are a few things to be aware of. So I mean, first, let's just have a quick look at the structure of our data. So when we're training a model for mass language modeling, we need a few tensors. We need three tensors. And this is for training Roberta, by the way, as well.\",\n",
       "  'id': '35Pdoyi6ZoQ-t15.84',\n",
       "  'url': 'https://youtu.be/35Pdoyi6ZoQ',\n",
       "  'published': '2021-07-06 13:00:03 UTC',\n",
       "  'channel_id': 'UCv83tO5cePwHMt1952IVVHw'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0], new_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "index_name = 'openai-youtube-transcriptions'\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv('PINECONE_API_KEY')\n",
    "index_name = 'openai-youtube-transcriptions'\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=len(res.data[0].embedding),\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',  \n",
    "            region='us-east-1'  \n",
    "        ),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 800}},\n",
       " 'total_vector_count': 800}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'deletion_protection': 'disabled',\n",
      " 'dimension': 1536,\n",
      " 'host': 'openai-youtube-transcriptions-4c82npc.svc.aped-4627-b74a.pinecone.io',\n",
      " 'metric': 'cosine',\n",
      " 'name': 'openai-youtube-transcriptions',\n",
      " 'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
      " 'status': {'ready': True, 'state': 'Ready'}}\n"
     ]
    }
   ],
   "source": [
    "# Another method to view index stats\n",
    "stats = pc.describe_index(index_name)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:21<00:00, 20.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 80  # how many embeddings we create and insert at once\n",
    "end_ = 800 # or len(new_data) 48000+\n",
    "for i in tqdm(range(0, 800, batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(1000, i+batch_size)\n",
    "    meta_batch = new_data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    res = openai.embeddings.create(input=texts, model=embed_model)\n",
    "    embeds = [record.embedding for record in res.data]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'start': x['start'],\n",
    "        'end': x['end'],\n",
    "        'title': x['title'],\n",
    "        'text': x['text'],\n",
    "        'url': x['url'],\n",
    "        'published': x['published'],\n",
    "        'channel_id': x['channel_id']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "query = (\n",
    "    \"Which training method should I use for sentence transformers when \" +\n",
    "    \"I only have pairs of related sentences?\"\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "res = openai.embeddings.create(\n",
    "    input=[query],\n",
    "    model=embed_model\n",
    ")\n",
    "\n",
    "# retrieve from Pinecone\n",
    "xq = res.data[0].embedding\n",
    "\n",
    "# get relevant contexts (including the questions)\n",
    "res = index.query(vector=xq, top_k=2, include_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'x1lAcT3xl5M-t534.0',\n",
       "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
       "                           'end': 578.0,\n",
       "                           'published': '2021-05-27 16:15:39 UTC',\n",
       "                           'start': 534.0,\n",
       "                           'text': 'that to our actual training data. '\n",
       "                                   \"Otherwise if it's just a single sentence \"\n",
       "                                   \"we don't actually add it to the training \"\n",
       "                                   'data. I mean ideally we would want to do '\n",
       "                                   'something like that but for this use case '\n",
       "                                   \"I don't want to get make things too \"\n",
       "                                   \"complicated. So the reason I'm doing that \"\n",
       "                                   'is for example this sentence is just a '\n",
       "                                   'single sentence in that paragraph and we '\n",
       "                                   \"can't guarantee that each continuous \"\n",
       "                                   'paragraph is talking about the same '\n",
       "                                   'subject you might switch. So for the sake '\n",
       "                                   \"of simplicity I'm just going to ignore the \"\n",
       "                                   'single sentence paragraphs although we do '\n",
       "                                   'have them in our bag so they can be pulled '\n",
       "                                   'in as potential sentence',\n",
       "                           'title': 'Training BERT #4 - Train With Next '\n",
       "                                    'Sentence Prediction (NSP)',\n",
       "                           'url': 'https://youtu.be/x1lAcT3xl5M'},\n",
       "              'score': 0.831959,\n",
       "              'values': []},\n",
       "             {'id': 'x1lAcT3xl5M-t516.0',\n",
       "              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',\n",
       "                           'end': 562.0,\n",
       "                           'published': '2021-05-27 16:15:39 UTC',\n",
       "                           'start': 516.0,\n",
       "                           'text': 'sentences and then we say if number of '\n",
       "                                   'sentences is greater than one oops OK '\n",
       "                                   \"don't excuse it right now and then we \"\n",
       "                                   'apply our 50-50 logic and append that to '\n",
       "                                   'our actual training data. Otherwise if '\n",
       "                                   \"it's just a single sentence we don't \"\n",
       "                                   'actually add it to the training data. I '\n",
       "                                   'mean ideally we would want to do something '\n",
       "                                   \"like that but for this use case I don't \"\n",
       "                                   'want to get make things too complicated. '\n",
       "                                   \"So the reason I'm doing that is for \"\n",
       "                                   'example this sentence is just a single '\n",
       "                                   \"sentence in that paragraph and we can't \"\n",
       "                                   'guarantee that each continuous paragraph',\n",
       "                           'title': 'Training BERT #4 - Train With Next '\n",
       "                                    'Sentence Prediction (NSP)',\n",
       "                           'url': 'https://youtu.be/x1lAcT3xl5M'},\n",
       "              'score': 0.825389564,\n",
       "              'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 6}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "\n",
    "def retrieve(query):\n",
    "    res = openai.embeddings.create(\n",
    "        input=[query],\n",
    "        model=embed_model\n",
    "    )\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res.data[0].embedding\n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(vector=xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below.\n",
      "\n",
      "Context:\n",
      "that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence\n",
      "\n",
      "---\n",
      "\n",
      "sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph\n",
      "\n",
      "---\n",
      "\n",
      "than one sentence. So we'll do number of sentences equals the length of sentences and then we say if number of sentences is greater than one oops OK don't excuse it right now and then we apply our 50-50 logic and append that to our actual training data. Otherwise if it's just a single sentence we don't actually add it to the training data. I mean ideally we would want to do something like that but for this use case I don't want to get make things too complicated.\n",
      "\n",
      "Question: Which training method should I use for sentence transformers when I only have pairs of related sentences?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "query_with_contexts = retrieve(query)\n",
    "print(query_with_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You should use the 50-50 logic and append the pairs of related sentences to the actual training data.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(query_with_contexts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
